{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thNgYzdVyfWM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "torch.manual_seed(12046)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 一些超参数\n",
        "learning_rate = 1e-3\n",
        "# 如果有GPU，该脚本将使用GPU进行计算\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "RYMfKksAzCox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset('Nan-Do/code-search-net-python')\n",
        "datasets = raw_datasets['train'].filter(lambda x: 'apache/spark' in x['repo'])"
      ],
      "metadata": {
        "id": "dvQq4boGzGVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.inputs_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(self.inputs_size + self.hidden_size, self.hidden_size)\n",
        "\n",
        "  def forward(self, input, hidden=None):\n",
        "    # input:  (1, I),在NLP领域，I等于文本嵌入的C\n",
        "    # hidden: (1, H)\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(input.device)\n",
        "    combined = torch.concat((input, hidden), dim=-1)  # (1, I + H)\n",
        "    hidden = F.relu(self.i2h(combined))    # (1,    H)\n",
        "    return hidden\n",
        "\n",
        "  def init_hidden(self, device):\n",
        "    return torch.zeros((1, self.hidden_size), device=device)"
      ],
      "metadata": {
        "id": "IdgtpBfczM72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_model = RNNCell(2, 3)\n",
        "data = torch.randn(4, 1, 2) # 四个句子，每个句子的的形状是(1, 2)\n",
        "hidden = None\n",
        "\n",
        "# 因为RNN是按序计算，不能并行，所以要手动循环每个句子\n",
        "for i in range(data.shape[0]):\n",
        "  hidden = r_model(data[i], hidden)\n",
        "  print(hidden)"
      ],
      "metadata": {
        "id": "ujy-u2G93lBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self, vs):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vs, 30)\n",
        "    self.rnn = RNNCell(30, 50)\n",
        "    self.lm = nn.Linear(50, vs)\n",
        "\n",
        "  def forward(self, x, hidden=None):\n",
        "    # x : (1)\n",
        "    # hidden : (1, 50)\n",
        "    embeddings = self.emb(x) # (1, 30)\n",
        "    hidden = self.rnn(embeddings, hidden)\n",
        "    out = self.lm(hidden) # (1, vs)\n",
        "    return out, hidden\n",
        "\n"
      ],
      "metadata": {
        "id": "0QBikKoyzGNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer:\n",
        "\n",
        "  def __init__(self, data, end_ind=0):\n",
        "    # data:list[str]\n",
        "    # 得到所有的字符\n",
        "    chars = sorted(list(set(''.join(data))))\n",
        "    self.char2ind = {s : i+1 for i, s in enumerate(chars)}\n",
        "    self.char2ind['<|e|>'] = end_ind\n",
        "    self.ind2char = {v : k for k, v in self.char2ind.items()}\n",
        "    self.end_ind = end_ind\n",
        "\n",
        "  def encode(self, x):\n",
        "    return [self.char2ind[c] for c in x]\n",
        "\n",
        "  def decode(self, x):\n",
        "    # x : int or list[int]\n",
        "    if isinstance(x, int):\n",
        "      return self.ind2char[x]\n",
        "    return [self.ind2char[i] for i in x]"
      ],
      "metadata": {
        "id": "-a-lqcai4wfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CharTokenizer(data=datasets['original_string'])\n",
        "test_str = 'def f(x):'\n",
        "encode_result = tokenizer.encode(test_str)\n",
        "decode_result = tokenizer.decode(encode_result)\n",
        "print(encode_result, ''.join(decode_result))"
      ],
      "metadata": {
        "id": "VmxWajmlxC2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_model = CharRNN(len(tokenizer.char2ind)).to(device)"
      ],
      "metadata": {
        "id": "ssI3wULDxlRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_model"
      ],
      "metadata": {
        "id": "hhhvxnI32CGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(tokenizer.encode('d'), device=device)\n",
        "out, hidden = c_model(inputs)\n",
        "out.shape, hidden.shape"
      ],
      "metadata": {
        "id": "yHZURQFR2TE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, idx, tokenizer, max_new_tokens=300):\n",
        "  # idx : (1)\n",
        "  out = idx.tolist()\n",
        "  hidden = None\n",
        "  model.eval()\n",
        "  for _ in range(max_new_tokens):\n",
        "    logits, hidden = model(idx, hidden)\n",
        "    probs = F.softmax(logits, dim=-1) #(1, 98)\n",
        "    # 随机生成文本\n",
        "    ix = torch.multinomial(probs, num_samples=1) # (1, 1)\n",
        "    out.append(ix.item())\n",
        "    idx = ix.squeeze(0)\n",
        "    if out[-1] == tokenizer.end_ind:\n",
        "      break\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "WCXn6XKL8hCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(tokenizer.encode('d'), device=device)\n",
        "''.join(tokenizer.decode(generate(c_model, inputs, tokenizer)))"
      ],
      "metadata": {
        "id": "ObG17Wck-Vas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(text, tokenizer):\n",
        "  # text : str\n",
        "  enc = tokenizer.encode(text)\n",
        "  inputs = enc\n",
        "  labels = enc[1:] + [tokenizer.end_ind]\n",
        "  return torch.tensor(inputs, device=device), torch.tensor(labels, device=device)"
      ],
      "metadata": {
        "id": "IrvXCwrO-3KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process(test_str, tokenizer)"
      ],
      "metadata": {
        "id": "JKC-sf7P_rbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossi = []\n",
        "epochs = 1\n",
        "optimizer = optim.Adam(c_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for e in range(epochs):\n",
        "  for data in datasets:\n",
        "    inputs, labels = process(data['original_string'], tokenizer)\n",
        "    hidden = None\n",
        "    _loss = 0.0\n",
        "    lens = len(inputs)\n",
        "    for i in range(lens):\n",
        "      logits, hidden = c_model(inputs[i].unsqueeze(0), hidden)\n",
        "      _loss += F.cross_entropy(logits, labels[i].unsqueeze(0)) / lens\n",
        "    lossi.append(_loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    _loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "jTfkhUvR_xqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SBPr860pJdSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(tokenizer.encode('d'), device=device)\n",
        "print(''.join(tokenizer.decode(generate(c_model, inputs, tokenizer))))"
      ],
      "metadata": {
        "id": "jT4vLMX0C-yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4_9lBVKJekf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "a0IfjgCmAi1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[1]['original_string']"
      ],
      "metadata": {
        "id": "H1ij0SjGJfMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZHKetW9JiZQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}