{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTI8fYpotA8u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "torch.manual_seed(12046)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 一些超参数\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 10\n",
        "batch_size=1000\n",
        "sequence_len=64\n",
        "# 如果有GPU，该脚本将使用GPU进行计算\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "AxVj08FmtdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"Nan-Do/code-search-net-python\")\n",
        "datasets = raw_datasets['train'].filter(lambda x: 'apache/spark' in x['repo'])"
      ],
      "metadata": {
        "id": "frsx43aatgnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharTokenizer:\n",
        "\n",
        "    def __init__(self, data, end_ind=0):\n",
        "        # data: list[str]\n",
        "        # 得到所有的字符\n",
        "        chars = sorted(list(set(''.join(data))))\n",
        "        self.char2ind = {s: i + 1 for i, s in enumerate(chars)}\n",
        "        self.char2ind['<|e|>'] = end_ind\n",
        "        self.ind2char = {v: k for k, v in self.char2ind.items()}\n",
        "        self.end_ind = end_ind\n",
        "\n",
        "    def encode(self, x):\n",
        "        # x: str\n",
        "        return [self.char2ind[i] for i in x]\n",
        "\n",
        "    def decode(self, x):\n",
        "        # x: int or list[x]\n",
        "        if isinstance(x, int):\n",
        "            return self.ind2char[x]\n",
        "        return [self.ind2char[i] for i in x]\n",
        "\n",
        "tokenizer = CharTokenizer(datasets['original_string'])"
      ],
      "metadata": {
        "id": "X1n8bSortjpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_str = 'def f(x):'\n",
        "re = tokenizer.encode(test_str)\n",
        "print(re)\n",
        "''.join(tokenizer.decode(range(len(tokenizer.char2ind))))"
      ],
      "metadata": {
        "id": "yPGGpg65uIcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "  def forward(self, input, hidden=None):\n",
        "    # input (B, T, C)\n",
        "    # hidden (B,  H)\n",
        "    # out (B, T, H)\n",
        "    B, T, C = input.shape\n",
        "    re = []\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(B, input.device)\n",
        "    for i in range(T):\n",
        "      combined = torch.concat((input[:, i, :], hidden), dim=-1) # (B, C+H)\n",
        "      hidden = self.i2h(combined) # (B, H)\n",
        "      re.append(hidden)\n",
        "    return torch.stack(re, dim=1) # (B, T, H)\n",
        "\n",
        "  def init_hidden(self, B, device):\n",
        "    return torch.zeros((B, self.hidden_size), device=device)"
      ],
      "metadata": {
        "id": "prKCho-OuZgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = RNN(3, 4)\n",
        "x = torch.randn(5, 2, 3)\n",
        "r(x).shape"
      ],
      "metadata": {
        "id": "wFJUfiCWypfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNNBatch(nn.Module):\n",
        "  def __init__(self, vs):\n",
        "    super().__init__()\n",
        "    emb_size = 256\n",
        "    hidden_size = 128\n",
        "    self.emb = nn.Embedding(vs, emb_size)\n",
        "    self.rnn1 = RNN(emb_size, hidden_size)\n",
        "    self.ln1 = nn.LayerNorm(hidden_size)\n",
        "    self.rnn2 = RNN(hidden_size, hidden_size)\n",
        "    self.ln2 = nn.LayerNorm(hidden_size)\n",
        "    self.lm = nn.Linear(hidden_size, vs)\n",
        "    self.dp = nn.Dropout(0.4)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x : (B, T)\n",
        "    B = x.shape[0]\n",
        "    embeddings = self.emb(x)  # (B, T, C)\n",
        "    h = F.relu(self.ln1(self.rnn1(embeddings))) # (B, T， H)\n",
        "    h = self.dp(h)\n",
        "    h = F.relu(self.ln2(self.rnn2(h)))      # (B, T, hidden_size)\n",
        "    h = self.dp(h)\n",
        "    out = self.lm(h)    # (B, T, vs)\n",
        "    return out"
      ],
      "metadata": {
        "id": "tSomWjVRHprY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_model = CharRNNBatch(len(tokenizer.char2ind)).to(device)\n",
        "c_model"
      ],
      "metadata": {
        "id": "4nD11LfGLcBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate(model, context, tokenizer, max_new_tokens=300):\n",
        "  # context : (1, T)\n",
        "  out = context.tolist()[0]\n",
        "  model.eval()\n",
        "  for _ in range(max_new_tokens):\n",
        "    logits = model(context)   # (1, T, vs98)\n",
        "    probs = F.softmax(logits[:, -1, :], dim=-1) # (1, vs98)\n",
        "    # 随机生成文本\n",
        "    ix = torch.multinomial(probs, num_samples=1) # (1, 1)\n",
        "    # 更新背景\n",
        "    context = torch.concat((context, ix), dim=-1)\n",
        "    out.append(ix.item())\n",
        "    if out[-1] == tokenizer.end_ind:\n",
        "      break\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "0miDJKQwLc5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor(tokenizer.encode('def'), device=device).unsqueeze(0)\n",
        "print(''.join(tokenizer.decode(generate(c_model, context, tokenizer))))"
      ],
      "metadata": {
        "id": "T96JO-mkNoih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(data, tokenizer, sequence_len=sequence_len):\n",
        "  text = data['original_string']\n",
        "  # text is list[str]\n",
        "  inputs, labels = [], []\n",
        "  for t in text:\n",
        "    enc = tokenizer.encode(t)\n",
        "    enc += [tokenizer.end_ind]\n",
        "    for i in range(len(enc) - sequence_len):\n",
        "      inputs.append(enc[i : i+sequence_len])\n",
        "      labels.append(enc[i+1 : i+1+sequence_len])\n",
        "  return {'inputs' : inputs, 'labels' : labels}"
      ],
      "metadata": {
        "id": "JGvBpg2WPIwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将数据分为训练集和测试集\n",
        "tokenized = datasets.train_test_split(test_size=0.1, seed=1024, shuffle=True)\n",
        "f = lambda x : process(x, tokenizer)\n",
        "tokenized = tokenized.map(f, batched=True, remove_columns=datasets.column_names)\n",
        "tokenized.set_format(type='torch', device=device)\n"
      ],
      "metadata": {
        "id": "nW-f_tsJPWYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized"
      ],
      "metadata": {
        "id": "ZELQAUifVMdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(tokenized['train'], batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(tokenized['test'], batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "NicgkxFZTaIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "id": "_SEZrO9TURiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "  re = {}\n",
        "  # 将模式切换至评估模式\n",
        "  model.eval()\n",
        "  re['train'] = _loss(model, train_loader)\n",
        "  re['test'] = _loss(model, test_loader)\n",
        "  # 将模型切换至训练模式\n",
        "  model.train()\n",
        "  return re\n",
        "\n",
        "\n",
        "def _loss(model, data_loader):\n",
        "  loss = []\n",
        "  data_iter = iter(data_loader)\n",
        "\n",
        "  # 随机使用多个批量数据来评估模型效果\n",
        "  for k in range(eval_iters):\n",
        "    data = next(data_iter)\n",
        "    if data is None:\n",
        "      data_iter = iter(data_loader)\n",
        "      data = next(data_iter)\n",
        "    inputs, labels = data['inputs'], data['labels'] # (B, T)\n",
        "    logits = model(inputs) # (B, T, vs)\n",
        "    loss.append(F.cross_entropy(logits.transpose(-2, -1), labels).item())\n",
        "  return torch.tensor(loss).mean().item()"
      ],
      "metadata": {
        "id": "oNP2kuS0UfAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_loss(c_model)"
      ],
      "metadata": {
        "id": "U8_s99hymvyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, epochs=10):\n",
        "  lossi = []\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "      inputs, labels = data['inputs'], data['labels'] # (B, T)\n",
        "      logits = model(inputs) # (B, T, vs)\n",
        "      optimizer.zero_grad()\n",
        "      loss = F.cross_entropy(logits.transpose(-2, -1), labels)\n",
        "      lossi.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    # 评估模型，并输出结果\n",
        "    stats = estimate_loss(model)\n",
        "    train_loss = f'train loss {stats[\"train\"]:.4f}'\n",
        "    test_loss = f'test loss {stats[\"test\"]:.4f}'\n",
        "    print(f'epoch {epoch:>2}: {train_loss}, {test_loss}')\n",
        "  return lossi\n"
      ],
      "metadata": {
        "id": "ZzzZoovomyip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = train_model(c_model, optim.Adam(c_model.parameters(), lr=learning_rate))"
      ],
      "metadata": {
        "id": "9HHVm1GIoha0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.tensor(tokenizer.encode('def'), device=device).unsqueeze(0)\n",
        "print(''.join(tokenizer.decode(generate(c_model, context, tokenizer))))"
      ],
      "metadata": {
        "id": "wvaz64D1pvP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.tensor(l).view(-1, 10).mean(dim=-1))"
      ],
      "metadata": {
        "id": "AvLFtnIcozn3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}