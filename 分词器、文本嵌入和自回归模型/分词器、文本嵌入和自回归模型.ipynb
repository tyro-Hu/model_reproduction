{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNQ52aBXjZqe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "torch.manual_seed(12046)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 一些超参数\n",
        "context_length = 10\n",
        "learning_rate = 0.01\n",
        "eval_iters = 10\n",
        "batch_size=1000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "1uP_TXIMjoUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset('Nan-Do/code-search-net-python')\n",
        "datasets = raw_datasets['train'].filter(lambda x: 'apache/spark' in x['repo'])\n",
        "# 通过索引提取datasets数据的时候，返回一个dict，其中的value是一个字符串\n",
        "print(datasets[8]['original_string'])\n",
        "# 当传入的是一个数组时，返回的依然是一个dict，但其中的value是一个列表\n",
        "print(datasets[8: 10]['original_string'])"
      ],
      "metadata": {
        "id": "-6i3ZgQvj99d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 手动实现tokenizer\n",
        "class char_tokenizer:\n",
        "\n",
        "  def __init__(self, data, begin_ind=0, end_ind=1):\n",
        "        # 数据中出现的所有字符构成字典\n",
        "        chars = sorted(list(set(''.join(data))))\n",
        "        # 预留两个位置给开头和结尾的特殊字符\n",
        "        self.char2ind = {s : i + 2 for i, s in enumerate(chars)}\n",
        "        self.char2ind['<|b|>'] = begin_ind\n",
        "        self.char2ind['<|e|>'] = end_ind\n",
        "        self.begin_ind = begin_ind\n",
        "        self.end_ind = end_ind\n",
        "        self.ind2char = {i : s for s, i in self.char2ind.items()}\n",
        "  # 实现tokenizer的编码器功能\n",
        "  def encode(self, text):\n",
        "        '''\n",
        "        编码\n",
        "        参数\n",
        "        ----\n",
        "        text ：str，文本\n",
        "        '''\n",
        "        return [self.char2ind[c] for c in text]\n",
        "\n",
        "  # 实现tokenizer的解码器功能\n",
        "  def decode(self, enc):\n",
        "        '''\n",
        "        解码\n",
        "        参数\n",
        "        ----\n",
        "        enc ：int or list[int]\n",
        "        '''\n",
        "        if isinstance(enc, int):\n",
        "            return self.ind2char[enc]\n",
        "        return [self.ind2char[i] for i in enc]\n"
      ],
      "metadata": {
        "id": "fzZq84QOlMPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 举例验证分词器\n",
        "tok = char_tokenizer(datasets['original_string'])\n",
        "example_text = 'def postappend(self):'\n",
        "''.join(tok.decode(tok.encode(example_text))), len(tok.char2ind)"
      ],
      "metadata": {
        "id": "bqunGPLHj_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 自回归转换，将文本转换成一系列的训练数据\n",
        "def autoregressive_trans(text, tokenizer, context_length=context_length):\n",
        "    '''\n",
        "    将文本转换成一系列的训练数据\n",
        "    参数\n",
        "    ----\n",
        "    text ：str，文本\n",
        "    tokenizer ：分词器\n",
        "    context_length ：int，背景文本的长度\n",
        "    返回\n",
        "    ----\n",
        "    inputs ：list[list[int]]，背景文本（特征）\n",
        "    labels ：list[list[int]]，预测标签\n",
        "    '''\n",
        "    inputs, labels = [], []\n",
        "    b_ind = tokenizer.begin_ind\n",
        "    e_ind = tokenizer.end_ind\n",
        "    enc = tokenizer.encode(text)\n",
        "    # 增加开始和结尾的特殊字符\n",
        "    x = [b_ind] * context_length + enc + [e_ind]\n",
        "    for i in range(len(x) - context_length):\n",
        "        inputs.append(x[i: i + context_length])\n",
        "        labels.append(x[i + context_length])\n",
        "    return inputs, labels"
      ],
      "metadata": {
        "id": "OwL4oyYXvObN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 举例展示自回归模式的训练数据\n",
        "inputs, labels = autoregressive_trans(example_text, tok)\n",
        "for a, b in zip(inputs, labels):\n",
        "  print(''.join(tok.decode(a)), '------>',  tok.decode(b))"
      ],
      "metadata": {
        "id": "g60EHt2tv66P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 对数据集中的所有数据进行处理\n",
        "def process(data):\n",
        "    '''\n",
        "    在datasets的map里使用，将文本转换成训练数据\n",
        "    '''\n",
        "    text = data['original_string']\n",
        "    # 如果是普通的map操作，传入的值是字符串\n",
        "    if isinstance(text, str):\n",
        "        inputs, labels = autoregressive_trans(text, tok)\n",
        "        return {'inputs': inputs, 'labels': labels}\n",
        "    # 如果是map操作里面batched=True，传入的值是字符串列表\n",
        "    inputs, labels = [], []\n",
        "    for i in text:\n",
        "        i, l = autoregressive_trans(i, tok)\n",
        "        inputs += i\n",
        "        labels += l\n",
        "    return {'inputs': inputs, 'labels': labels}"
      ],
      "metadata": {
        "id": "0RAL8fNWwAp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试process的功能\n",
        "process(datasets[8:9])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O-rUXlNzI4jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将数据分为训练集和测试集\n",
        "tokenized = datasets.train_test_split(test_size=0.1, shuffle=True, seed=1024)\n",
        "# 将文本转换为训练数据，里面包含inputs和labels\n",
        "tokenized = tokenized.map(process, batched=True, remove_columns=datasets.column_names)\n",
        "tokenized.set_format(type='torch', device=device)\n"
      ],
      "metadata": {
        "id": "Smhlz7k8Pih9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建数据加载器\n",
        "train_loader = DataLoader(tokenized['train'], batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(tokenized['test'], batch_size=batch_size, shuffle=True)\n",
        "# 获取一个批量的数据\n",
        "next(iter(test_loader))\n"
      ],
      "metadata": {
        "id": "FnfR-e4lWQ2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义模型\n",
        "class CharMLP(nn.Module):\n",
        "  def __init__(self, vs):\n",
        "    super().__init__()\n",
        "    '''\n",
        "    根据文本背景预测下一个字母是什么\n",
        "    参数\n",
        "    ----\n",
        "    vs ：int，字典大小\n",
        "    '''\n",
        "    # 文字嵌入层\n",
        "    self.embedding= nn.Embedding(vs, 30)\n",
        "    self.hidden1 = nn.Linear(300, 200)\n",
        "    self.hidden2 = nn.Linear(200, 100)\n",
        "    self.out = nn.Linear(100, vs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    向前传播\n",
        "    参数\n",
        "    ----\n",
        "    x ：torch.LongTensor，背景文本，其中的元素表示相应位置的字母在字典中的位置\n",
        "    返回\n",
        "    ----\n",
        "    h ：torch.FloatTensor，预测结果的logits\n",
        "    '''\n",
        "    # 因为背景文本的长度（context_length）等于10，\n",
        "    # 所以x的形状是(B, 10)，B表示批量数据的大小\n",
        "    B = x.shape[0]           # (B,  10)\n",
        "    emb = self.embedding(x)      # (B,  10, 30)\n",
        "    h = emb.view(B, -1)       # (B, 300)\n",
        "    h = F.relu(self.hidden1(h))    # (B, 200)\n",
        "    h = F.relu(self.hidden2(h))    # (B, 100)\n",
        "    h = self.out(h)          # (B,  vs)\n",
        "    return h\n",
        "\n"
      ],
      "metadata": {
        "id": "UvcMrgHuZzWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharMLP(len(tok.char2ind)).to(device)"
      ],
      "metadata": {
        "id": "k-j13eXLiJwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "@torch.no_grad()\n",
        "def generate(model, context, max_new_token=300):\n",
        "  '''\n",
        "  利用模型生成文本（反复使用模型进行预测）\n",
        "  参数\n",
        "  ----\n",
        "  model ：CharMLP，生成文本的模型\n",
        "  context ：torch.LongTensor，背景文本，形状为(1, 10)\n",
        "  max_new_tokens ：int，生成文本的最大长度\n",
        "  返回\n",
        "  ----\n",
        "  out ：list[int]，生成的文本\n",
        "  '''\n",
        "  out = []\n",
        "  # 将模型切换到评估模式\n",
        "  model.eval()\n",
        "  for _ in range(max_new_token):\n",
        "    logits = model(context)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # 根据模型预测的概率，得到最终的预测结果（下一个字母）\n",
        "    # 这一步运算有一定随机性\n",
        "    ix = torch.multinomial(probs, num_samples=1)\n",
        "    # 利用模型的预测结果更新文本背景\n",
        "    context = torch.cat((context[:, 1:], ix), dim=1)\n",
        "    out.append(ix.item())\n",
        "    if ix.item() == tok.end_ind:\n",
        "      break\n",
        "  # 将模型切换至训练模式\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "kA1hibSCimkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用模型来生成文本\n",
        "context = torch.zeros((1, 10), dtype=torch.long, device=device)\n",
        "print(''.join(tok.decode(generate(model, context))))"
      ],
      "metadata": {
        "id": "Is6Q9Hcfi2qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 评估模型\n",
        "@torch.no_grad()\n",
        "def _loss(model, data_loader):\n",
        "  loss = []\n",
        "  data_iter = iter(data_loader)\n",
        "\n",
        "  # 随机使用多个批量数据来预估模型效果\n",
        "  for k in range(eval_iters):\n",
        "    data = next(data_iter, None)\n",
        "    if data is None:\n",
        "      data_iter = iter(data_loader)   # 重新创建一个新的 DataLoader 迭代器\n",
        "      data = next(data_iter, None)    # 再取一个 batch\n",
        "    inputs, labels = data['inputs'], data['labels']\n",
        "    logits = model(inputs)\n",
        "    loss.append(F.cross_entropy(logits, labels).item())\n",
        "  return torch.tensor(loss).mean().item()\n",
        "\n",
        "\n",
        "def estimate_loss(model):\n",
        "  re = {}\n",
        "  # 将模型切换到评估模式\n",
        "  model.eval()\n",
        "  re['train'] = _loss(model, train_loader)\n",
        "  re['test'] = _loss(model, test_loader)\n",
        "  # 将模型切换至训练模式\n",
        "  model.train()\n",
        "  return re\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "zQm5NEzDspwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_loss(model)"
      ],
      "metadata": {
        "id": "gRSsp0QBv7s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp(model, optimizer, data_loader, epochs=10):\n",
        "  # 记录模型在训练集上的模型损失\n",
        "  lossi = []\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(data_loader, 0):\n",
        "      inputs, labels = data['inputs'], data['labels']\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(inputs)\n",
        "      loss = F.cross_entropy(logits, labels)\n",
        "      lossi.append(loss.item())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    # 评估模型，并输出结果\n",
        "    stats = estimate_loss(model)\n",
        "    train_loss = f'train loss {stats[\"train\"]:.4f}'\n",
        "    test_loss = f'test loss {stats[\"test\"]:.4f}'\n",
        "    print(f'epoch {epoch:>2}: {train_loss}, {test_loss}')\n",
        "  return lossi\n"
      ],
      "metadata": {
        "id": "hNx2rGRwr2Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = train_mlp(model, optim.Adam(model.parameters(), lr=learning_rate), train_loader)"
      ],
      "metadata": {
        "id": "vDLA9jATv14s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.tensor(l).view(-1, 10).mean(1).numpy())"
      ],
      "metadata": {
        "id": "6CaeRhYev7M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用模型来生成文本\n",
        "context = torch.zeros((1, 10), dtype=torch.long, device=device)\n",
        "print(''.join(tok.decode(generate(model, context))))"
      ],
      "metadata": {
        "id": "xNHdc_PFxVjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-ZCzFz2zzJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}